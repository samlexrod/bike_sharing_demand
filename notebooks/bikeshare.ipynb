{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bikeshare Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from src import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are provided hourly rental data spanning two years. For this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. You must predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period.\n",
    "\n",
    "Data Fields\n",
    "- datetime - hourly date + timestamp  \n",
    "- season \n",
    "    - 1 = spring, \n",
    "    - 2 = summer, \n",
    "    - 3 = fall, \n",
    "    - 4 = winter \n",
    "- holiday - whether the day is considered a holiday\n",
    "- workingday - whether the day is neither a weekend nor holiday\n",
    "- weather \n",
    "    - 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "    - 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "    - 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "    - 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \n",
    "- temp - temperature in Celsius\n",
    "- atemp - \"feels like\" temperature in Celsius\n",
    "- humidity - relative humidity\n",
    "- windspeed - wind speed\n",
    "- casual - number of non-registered user rentals initiated\n",
    "- registered - number of registered user rentals initiated\n",
    "- count - number of total rentals <- PREDICTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training without Using Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader()\n",
    "\n",
    "loader.load_feature_engineered(\"no_data_engineering\")\n",
    "\n",
    "# Loading raw trained and test data to process without any changes\n",
    "train_df, test_df = loader.get_train_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Training without New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = TabularPredictor(\n",
    "        label='count',\n",
    "        path='autogluon',\n",
    "        eval_metric='root_mean_squared_error',\n",
    "    ).fit(\n",
    "        train_df,\n",
    "        time_limit=600,\n",
    "        presets='best_quality'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Showing the Leatherboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    display(predictor.leaderboard(silent=True))\n",
    "except:\n",
    "    predictor = TabularPredictor.load('autogluon')\n",
    "    display(predictor.leaderboard(silent=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Predictions on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictor.predict(test_df)\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying negative predictions\n",
    "predictions.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting negative predictions\n",
    "negative_prediction_count = (predictions < 0).sum()\n",
    "\n",
    "print(f\"There are {negative_prediction_count} negative predictions to set to zero.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Setting up for Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting negative predictions to zero\n",
    "predictions[predictions < 0] = 0\n",
    "\n",
    "predictions.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.read_csv('../data/sampleSubmission.csv')\n",
    "display(submission_df.head())\n",
    "submission_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df['count'] = predictions\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "display(submission_df.head())\n",
    "submission_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Submitting Initial Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the best model name\n",
    "best_model = predictor.model_best\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already submitted\n",
    "# !kaggle competitions submit -c bike-sharing-demand -f submission.csv -m f\"irst submission with {best_model}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions submissions -c bike-sharing-demand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Extract Feature Engineered Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader()\n",
    "\n",
    "loader.load_feature_engineered(\"no_data_engineering\")\n",
    "# Loading raw trained and test data with changes to the season and weather columns\n",
    "loader.set_as_category(columns=[\"season\", \"weather\"])\n",
    "train_df, test_df = loader.get_train_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Re-training the model with categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_new_features = TabularPredictor(\n",
    "    label='count',\n",
    "    path='autogluon-new-features',\n",
    "    eval_metric='root_mean_squared_error',\n",
    ").fit(\n",
    "    train_df,\n",
    "    time_limit=600,\n",
    "    presets='best_quality'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Showing the Leatherboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    display(predictor_new_features.leaderboard(silent=True))\n",
    "except:\n",
    "    predictor_new_features = TabularPredictor.load('autogluon-new-features')\n",
    "    display(predictor_new_features.leaderboard(silent=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Predictions on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_new_features = predictor_new_features.predict(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Setting up for Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting negative predictions to zero\n",
    "predictions_new_features[predictions_new_features < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_new_features_df = pd.read_csv('../data/sampleSubmission.csv')\n",
    "submission_new_features_df['count'] = predictions_new_features\n",
    "submission_new_features_df.to_csv('submission-new-features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare old and new submissions\n",
    "submission_df = pd.read_csv('submission.csv')\n",
    "submission_new_features_df = pd.read_csv('submission-new-features.csv')\n",
    "\n",
    "submission_df.merge(submission_new_features_df, on='datetime', suffixes=('_old', '_new'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Submitting New Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = predictor_new_features.model_best\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already submitted\n",
    "# !kaggle competitions submit -c bike-sharing-demand -f submission-new-features.csv -m \"new features with {best_model}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions submissions -c bike-sharing-demand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with Hyperparameter Tuning\n",
    "The following documentation show how to train a model with hyperparameter tuning using AutoGluon.\n",
    "\n",
    "Documentation: https://auto.gluon.ai/stable/api/autogluon.tabular.TabularPredictor.fit.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Transforming Features as Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to categorical for better performance\n",
    "loader = DataLoader()\n",
    "\n",
    "loader.load_feature_engineered(checkpoint_name='hyperparameter_tuning')\n",
    "loader.set_as_category(columns=[\"season\", \"weather\"])\n",
    "\n",
    "train_df, test_df = loader.get_train_test_data()\n",
    "\n",
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Creating a Validation Set for Local Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_val_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\"\" \n",
    "Train shape: {train_val_df.shape}\n",
    "Validation shape: {val_df.shape}    \n",
    "\"\"\")\n",
    "\n",
    "train_val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Re-train with One Hot Encoding & Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular.configs.hyperparameter_configs import get_hyperparameter_config\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from autogluon.common import space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'NN_TORCH': {'num_epochs': 10, 'activation': 'relu', 'dropout_prob': space.Real(0.0, 0.5)},\n",
    "    'GBM': {'num_boost_round': 1000, 'learning_rate': space.Real(0.01, 0.1, log=True)},\n",
    "    'XGB': {'n_estimators': 1000, 'learning_rate': space.Real(0.01, 0.1, log=True)}\n",
    "}\n",
    "\n",
    "\n",
    "hyper_timeout = 1 * 60  # seconds\n",
    "time_limit = 3 * 60\n",
    "print(f\"Hyperparameter optimization time: {hyper_timeout/60} minutes\")\n",
    "print(f\"Time limit: {time_limit/60} minutes\")\n",
    "\n",
    "# Custom hyperparameter tuning configuration\n",
    "hyperparameter_tune_kwargs = {\n",
    "    'num_trials': 20,  # Number of trials to run\n",
    "    'scheduler': 'local',  # Scheduler to use for parallel training\n",
    "    'searcher': 'bayes',  # Searcher to use for hyperparameter optimization\n",
    "    'time_out': hyper_timeout,  # Time limit in seconds for each call to the ML model\n",
    "}\n",
    "\n",
    "predictor_new_hpo = TabularPredictor(\n",
    "    label='count',\n",
    "    path='autogluon-new-hpo',\n",
    "    eval_metric='root_mean_squared_error'    \n",
    ")\n",
    "\n",
    "predictor_new_hpo.fit(\n",
    "    train_val_df,\n",
    "    time_limit=time_limit,\n",
    "    presets='best_quality',\n",
    "    hyperparameters=hyperparameters,\n",
    "    hyperparameter_tune_kwargs=hyperparameter_tune_kwargs,\n",
    "    num_cpus=6,\n",
    "    num_gpus=1,\n",
    "    num_stack_levels=3,\n",
    "    verbosity=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_new_hpo.leaderboard(silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = predictor_new_hpo.evaluate(val_df)\n",
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_new_hpo = predictor_new_hpo.predict(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace negative predictions with zero\n",
    "predictions_new_hpo[predictions_new_hpo < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating scores of predictions\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "mean_squared_log_error(val_df[\"count\"], predictions_new_hpo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Loading the Best Model and Predicting on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = predictor_new_hpo.model_best\n",
    "print(f\"The best model is {best_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_predictor = TabularPredictor.load('autogluon-new-hpo')\n",
    "saved_predictor.leaderboard(silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_tunning_prediction_df = saved_predictor.predict(test_df)\n",
    "\n",
    "# Replace negative predictions with zero\n",
    "hyper_tunning_prediction_df[hyper_tunning_prediction_df < 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Submitting Fine Tuned Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission\n",
    "submission_hyper_tunning_df = pd.read_csv('../data/sampleSubmission.csv')\n",
    "submission_hyper_tunning_df['count'] = hyper_tunning_prediction_df\n",
    "submission_hyper_tunning_df.to_csv('submission-hyper-tunning.csv', index=False)\n",
    "\n",
    "!kaggle competitions submit -c bike-sharing-demand -f submission-hyper-tunning.csv -m \"hyperparameter tunning with {best_model}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions submissions -c bike-sharing-demand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working: Training with Hyperparameter Tuning & Extra Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader.load_feature_engineered(\"extra_feature_engineering\")\n",
    "loader.set_as_category(columns=[\"season\", \"weather\"])\n",
    "\n",
    "train_df, test_df = loader.get_train_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Creating Validation Set for Local Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_val_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\"\" \n",
    "Train shape: {train_val_df.shape}\n",
    "Validation shape: {val_df.shape}  \n",
    "Test shape: {test_df.shape}  \n",
    "\"\"\")\n",
    "\n",
    "train_val_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Re-train with Extra Features & Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular.configs.hyperparameter_configs import get_hyperparameter_config\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from autogluon.common import space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'NN_TORCH': {'num_epochs': 10, 'activation': 'relu', 'dropout_prob': space.Real(0.0, 0.5)},\n",
    "    'GBM': {'num_boost_round': 1000, 'learning_rate': space.Real(0.01, 0.1, log=True)},\n",
    "    'XGB': {'n_estimators': 1000, 'learning_rate': space.Real(0.01, 0.1, log=True)}\n",
    "}\n",
    "\n",
    "\n",
    "hyper_timeout = 1 * 60  # seconds\n",
    "# time_limit = 15 * 60\n",
    "print(f\"Hyperparameter optimization time: {hyper_timeout/60} minutes\")\n",
    "print(f\"Time limit: {time_limit/60} minutes\")\n",
    "\n",
    "# Custom hyperparameter tuning configuration\n",
    "hyperparameter_tune_kwargs = {\n",
    "    'num_trials': 20,  # Number of trials to run\n",
    "    'scheduler': 'local',  # Scheduler to use for parallel training\n",
    "    'searcher': 'bayes',  # Searcher to use for hyperparameter optimization\n",
    "    'time_out': hyper_timeout,  # Time limit in seconds for each call to the ML model\n",
    "}\n",
    "\n",
    "predictor_extra_hpo = TabularPredictor(\n",
    "    label='count',\n",
    "    path='autogluon-extra-hpo',\n",
    "    eval_metric='root_mean_squared_error'    \n",
    ")\n",
    "\n",
    "predictor_extra_hpo.fit(\n",
    "    train_val_df,\n",
    "    # time_limit=time_limit,\n",
    "    presets='best_quality',\n",
    "    hyperparameters=hyperparameters,\n",
    "    hyperparameter_tune_kwargs=hyperparameter_tune_kwargs,\n",
    "    num_cpus=6,\n",
    "    num_gpus=1,\n",
    "    num_stack_levels=3,\n",
    "    verbosity=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_extra_hpo.leaderboard(silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = predictor_extra_hpo.evaluate(val_df)\n",
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_extra_hpo = predictor_extra_hpo.predict(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace negative predictions with zero\n",
    "predictions_extra_hpo[predictions_extra_hpo < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating scores of predictions\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "mean_squared_log_error(val_df[\"count\"], predictions_extra_hpo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Loading the Best Model and Predicting on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = predictor_extra_hpo.model_best\n",
    "print(f\"The best model is {best_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_predictor = TabularPredictor.load('autogluon-extra-hpo')\n",
    "saved_predictor.leaderboard(silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_tunning_prediction_df = saved_predictor.predict(test_df)\n",
    "\n",
    "# Replace negative predictions with zero\n",
    "hyper_tunning_prediction_df[hyper_tunning_prediction_df < 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Submitting Fine Tuned Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission\n",
    "submission_hyper_tunning_df = pd.read_csv('../data/sampleSubmission.csv')\n",
    "submission_hyper_tunning_df['count'] = hyper_tunning_prediction_df\n",
    "submission_hyper_tunning_df.to_csv('submission-hyper-tunning.csv', index=False)\n",
    "\n",
    "!kaggle competitions submit -c bike-sharing-demand -f submission-hyper-tunning.csv -m \"hyperparameter tunning with extra features {best_model}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions submissions -c bike-sharing-demand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Including Custom Models on Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader.load_feature_engineered(checkpoint_name='hyperparameter_tuning')\n",
    "loader.set_as_category(columns=[\"season\", \"weather\"])\n",
    "\n",
    "train_df, test_df = loader.get_train_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Creating Validation Set for Local Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_val_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\"\" \n",
    "Train shape: {train_val_df.shape}\n",
    "Validation shape: {val_df.shape}  \n",
    "Test shape: {test_df.shape}  \n",
    "\"\"\")\n",
    "\n",
    "train_val_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Random Forest Regressor\n",
    "\n",
    "TODO: Apply advanced ensemble techniques to improve the model performance.\n",
    "\n",
    "Here is a list of ensemble techniques that you can use to improve the model performance:\n",
    "- Stacking: Stacked Generalization\n",
    "- Blending: Weighted Average\n",
    "- Bagging: Bootstrap Aggregating\n",
    "- Boosting: AdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Random Forest Regressor model\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Enhanced parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "\n",
    "# Set up GridSearchCV with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    model,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit the grid search with your training data\n",
    "grid_search.fit(train_val_df.drop(columns=['count', 'date']), train_val_df['count'])\n",
    "\n",
    "# Output the best parameters\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Use the best model to make predictions on the validation/test set\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Prepare validation data for predictions\n",
    "X_val = val_df.drop(columns=['count', 'date'])  # Features from validation data\n",
    "y_val = val_df['count']  # True target values\n",
    "\n",
    "# Predict on validation data using the best model\n",
    "predictions = best_model.predict(X_val)\n",
    "\n",
    "# Replace negative predictions with zero (if required for your use case)\n",
    "predictions[predictions < 0] = 0\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_val, predictions)\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_val, predictions)\n",
    "\n",
    "# Calculate R² score\n",
    "r2 = r2_score(y_val, predictions)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"R² Score: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Random Forest Regressor model\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Enhanced parameter distribution (same as param_grid but for RandomizedSearchCV)\n",
    "param_distributions = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Set up RandomizedSearchCV with cross-validation\n",
    "random_search = RandomizedSearchCV(\n",
    "    model,\n",
    "    param_distributions,\n",
    "    n_iter=50,  # Number of random combinations to try\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    n_jobs=-1,  # Use all available processors\n",
    "    verbose=2,  # Verbosity level\n",
    "    random_state=42  # Ensures reproducibility\n",
    ")\n",
    "\n",
    "# Fit the RandomizedSearchCV with your training data\n",
    "random_search.fit(train_val_df.drop(columns=['count', 'date']), train_val_df['count'])\n",
    "\n",
    "# Output the best parameters\n",
    "print(f\"Best parameters: {random_search.best_params_}\")\n",
    "\n",
    "# Use the best model to make predictions on the validation/test set\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Prepare validation data for predictions\n",
    "X_val = val_df.drop(columns=['count', 'date'])  # Features from validation data\n",
    "y_val = val_df['count']  # True target values\n",
    "\n",
    "# Predict on validation data using the best model\n",
    "predictions = best_model.predict(X_val)\n",
    "\n",
    "# Replace negative predictions with zero (if required for your use case)\n",
    "predictions[predictions < 0] = 0\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_val, predictions)\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_val, predictions)\n",
    "\n",
    "# Calculate R² score\n",
    "r2 = r2_score(y_val, predictions)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"R² Score: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Regressor (Abandoned)\n",
    "> The KNN Regressor was abandoned due to performing poorly on the dataset compared to the AutoGluon Weighted Ensembles L2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_log_error, r2_score\n",
    "\n",
    "# Define the KNN model\n",
    "base_knn = KNeighborsRegressor()\n",
    "\n",
    "# Wrap the KNN regressor inside a Bagging Regressor\n",
    "model = BaggingRegressor(estimator=base_knn, n_estimators=50, n_jobs=-1, random_state=42)\n",
    "\n",
    "# Define hyperparameter grid for the base estimator (KNN)\n",
    "param_grid = {\n",
    "    'estimator__n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'estimator__weights': ['uniform', 'distance'],\n",
    "    'estimator__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'estimator__leaf_size': [10, 20, 30],\n",
    "    'estimator__p': [1, 2]  # Minkowski distance: p=1 (Manhattan), p=2 (Euclidean)\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    model,\n",
    "    param_grid,\n",
    "    cv=10,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Normalize the training data\n",
    "scaler = StandardScaler()\n",
    "train_val_df_scaled = scaler.fit_transform(train_val_df.drop(columns=['count', 'date']))\n",
    "\n",
    "# Fit the GridSearchCV with scaled training data\n",
    "grid_search.fit(train_val_df_scaled, train_val_df['count'])\n",
    "\n",
    "# Output the best parameters from GridSearchCV\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Scale the validation data using the same scaler\n",
    "val_df_scaled = scaler.transform(val_df.drop(columns=['count', 'date']))\n",
    "\n",
    "# Predict on the scaled validation data\n",
    "predictions = grid_search.predict(val_df_scaled)\n",
    "\n",
    "# Replace negative predictions with zero\n",
    "predictions[predictions < 0] = 0\n",
    "\n",
    "# Ensure there are no zero values in target when calculating mean_squared_log_error\n",
    "msle = mean_squared_log_error(val_df[\"count\"] + 1e-10, predictions + 1e-10)\n",
    "r2 = r2_score(val_df[\"count\"], predictions)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Mean Squared Log Error: {msle}\")\n",
    "print(f\"R2 Score: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_log_error, r2_score\n",
    "\n",
    "# Define the KNN model\n",
    "base_knn = KNeighborsRegressor()\n",
    "\n",
    "# Wrap the KNN regressor inside a Bagging Regressor\n",
    "model = BaggingRegressor(estimator=base_knn, n_estimators=50, n_jobs=-1, random_state=42)\n",
    "\n",
    "# Define hyperparameter distributions for the base estimator (KNN)\n",
    "param_distributions = {\n",
    "    'estimator__n_neighbors': [1, 3, 5, 7, 9, 11, 13, 15, 20, 25],\n",
    "    'estimator__weights': ['uniform', 'distance'],\n",
    "    'estimator__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'estimator__leaf_size': [10, 20, 30, 40, 50],\n",
    "    'estimator__p': [1, 2]  # Minkowski distance: p=1 (Manhattan), p=2 (Euclidean)\n",
    "}\n",
    "\n",
    "# Set up RandomizedSearchCV with cross-validation\n",
    "random_search = RandomizedSearchCV(\n",
    "    model,\n",
    "    param_distributions,\n",
    "    n_iter=50,\n",
    "    cv=10,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Normalize the training data\n",
    "scaler = StandardScaler()\n",
    "train_val_df_scaled = scaler.fit_transform(train_val_df.drop(columns=['count', 'date']))\n",
    "\n",
    "# Fit the RandomizedSearchCV with scaled training data\n",
    "random_search.fit(train_val_df_scaled, train_val_df['count'])\n",
    "\n",
    "# Output the best parameters from RandomizedSearchCV\n",
    "print(f\"Best parameters: {random_search.best_params_}\")\n",
    "\n",
    "# Scale the validation data using the same scaler\n",
    "val_df_scaled = scaler.transform(val_df.drop(columns=['count', 'date']))\n",
    "\n",
    "# Predict on the scaled validation data\n",
    "predictions = random_search.predict(val_df_scaled)\n",
    "\n",
    "# Replace negative predictions with zero\n",
    "predictions[predictions < 0] = 0\n",
    "\n",
    "# Ensure there are no zero values in target when calculating mean_squared_log_error\n",
    "msle = mean_squared_log_error(val_df[\"count\"] + 1e-10, predictions + 1e-10)\n",
    "r2 = r2_score(val_df[\"count\"], predictions)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Mean Squared Log Error: {msle}\")\n",
    "print(f\"R2 Score: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Analysis of Custom Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rubric Validation\n",
    "\n",
    "Here's the rubric in bullet point format:\n",
    "\n",
    "### Loading the Dataset\n",
    "\n",
    "- **Download the Bike Sharing Demand data from Kaggle:**\n",
    "  - <input type='checkbox' checked/> Student uses the Kaggle CLI with the Kaggle API token to download and unzip the Bike Sharing Demand dataset into Sagemaker Studio (or local development). \n",
    "  \n",
    "- **Load all datasets from the Bike Sharing Demand competition into Pandas:**\n",
    "  - <input type='checkbox' checked/> Student uses Pandas' `read_csv()` function to load the train, test, and sample submission files into DataFrames.\n",
    "  - <input type='checkbox' checked/> Once loaded, the DataFrames can be viewed in the Jupyter notebook.\n",
    "\n",
    "### Feature Creation and Data Analysis\n",
    "\n",
    "- **Create a feature and add it to the train and test dataset:**\n",
    "  - <input type='checkbox' checked> Student extracts data from one feature column to create a new feature column in both the train and test datasets.\n",
    "\n",
    "- **Create a histogram of all features in the train dataset:**\n",
    "  - <input type='checkbox' checked> Student creates a Matplotlib image showing histograms of each feature column in the train DataFrame.\n",
    "\n",
    "- **Change the datatype of features in the train and test dataset:**\n",
    "  - <input type='checkbox' checked> Student assigns categorical data types to feature columns that were initially typed as numeric values.\n",
    "\n",
    "### Model Training With AutoGluon\n",
    "\n",
    "- **Train a Tabular Prediction model on the training set:**\n",
    "  - <input type='checkbox' checked> Student uses the `TabularPredictor` class from AutoGluon to create a predictor by calling `.fit()`.\n",
    "\n",
    "- **Change the hyperparameters when training a Tabular Prediction model:**\n",
    "  - <input type='checkbox' checked> Student provides additional arguments in the `TabularPredictor .fit()` function to adjust hyperparameters during training.\n",
    "\n",
    "- **Make predictions with a trained model on a test dataset:**\n",
    "  - <input type='checkbox' checked> Student uses the predictor created by fitting a model with `TabularPredictor` to predict new values from the test dataset.\n",
    "\n",
    "### Compare Model Performance\n",
    "\n",
    "- **Submit a prediction submission from a model to Kaggle for scoring:**\n",
    "  - <input type='checkbox' checked> Student uses the Kaggle CLI to submit their predictions from the trained AutoGluon Tabular Predictor to Kaggle for public score submission.\n",
    "\n",
    "- **Graph changes in their model evaluation metric after each model iteration:**\n",
    "  - Student uses Matplotlib or Google Sheets/Excel to chart model performance metrics in a line chart.\n",
    "  - The metric is derived from either `fit_summary()` or `leaderboard()` of the predictor.\n",
    "  - Y-axis: metric number; X-axis: each model iteration.\n",
    "\n",
    "- **Graph changes to their Kaggle competition score after each model iteration:**\n",
    "  - Student uses Matplotlib or Google Sheets/Excel to chart changes in the competition score.\n",
    "  - Y-axis: Kaggle score; X-axis: each model iteration.\n",
    "\n",
    "### Competition Report\n",
    "\n",
    "- **Identify which model from AutoGluon performed the best from fitting the train data to the Tabular Predictor:**\n",
    "  - The report uses `fit_summary()` or `leaderboard()` to detail the results of the training run, indicating the best model as the first entry.\n",
    "\n",
    "- **Show how doing EDA led to discoveries in the data that impacted model performance:**\n",
    "  - The report discusses how adding additional features and changing hyperparameters directly improved the Kaggle score.\n",
    "\n",
    "- **Explain why changes to hyperparameters affected the outcome of the model’s performance:**\n",
    "  - The report contains a table outlining each hyperparameter used along with the corresponding Kaggle score for each iteration.\n",
    "  - The report explains why specific changes to a hyperparameter affected the model's performance outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF NOTEBOOK"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bikeshare",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
